# 初期
$ cd ~

# python install
$ sudo apt install -y python3-pip
$ python3 -m pip install llama-cpp-python==0.2.77 --prefer-binary  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122
$ pip3 install torch==2.3.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/test/cpu

# 環境変数を設定
$ set LLAMA_CUBLAS=1
$ set CMAKE_ARGS=-DLLAMA_CUBLAS=on
$ set FORCE_CMAKE=1

# model install
$ git clone https://github.com/pekoka/llm-lama-cpu.git
$ cd llm-lama-cpu
$ wget -P ./model_assets/LocalLLM https://huggingface.co/TheBloke/Swallow-13B-Instruct-GGUF/resolve/main/swallow-13b-instruct.Q4_K_M.gguf?download=true

